{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "import itertools\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    text = re.sub(\"n\", ' ', text)\n",
    "\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    #print(tokens)\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "                #print(temp_cach)\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "                #print(w,' : ',temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    #print(words_lem_without_stopwords)\n",
    "    return words_lem_without_stopwords\n",
    "\n",
    "def get_lda_vector(text):\n",
    "    '''\n",
    "    векторное представление новости\n",
    "    '''\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(N_topic):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('articles.csv')\n",
    "users = pd.read_csv('users_arrticles.csv')\n",
    "target = pd.read_csv('users_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/antonromaskin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_ru = stopwords.words('russian')\n",
    "\n",
    "with open('stopwords.txt') as f:\n",
    "    add_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_ru += add_stopwords\n",
    "len(stopword_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 172 ms, total: 22.7 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# очистка текста\n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 374 ms, total: 3min 10s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# лемматизация текста\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#с формируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]\n",
    "\n",
    "# создаём коллекцию текстов\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем разбивать текст на 10 тем\n",
    "N_topic = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.1 s, sys: 3.69 s, total: 47.8 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "# импортируем модель\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем модель на коллекции текстов\n",
    "lda = LdaModel(common_corpus, num_topics=N_topic, id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем модель на диск\n",
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка ммодели с диска\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание новой коллекции из ранее не встречавшихся документов\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: газета ru рынок новый санкция сотрудник всё первый глава снижение решение доход экономика власть газпром\n",
      "topic_1: фонд вода остров снимок рейтинг озеро следователь флот планета британский сосед бежать океан польский польша\n",
      "topic_2: россия российский сша украина американский правительство рост сторона глава космический журнал эксперт нефть новый вопрос\n",
      "topic_3: рубль млн банк млрд руб составить клетка цена тыс размер валюта сумма статья стоимость тело\n",
      "topic_4: фестиваль гражданин конкурс дональд достигать хороший тенденция туроператор устойчивый игра россиянин тур величина египетский диск\n",
      "topic_5: ракета марс египет превысить пуск соцсеть журнал звёздный понятие запуск es модификация спешить перо каир\n",
      "topic_6: ребёнок газ всё день мужчина исследователь жизнь выяснить семья случай врач обнаружить найти произойти изз\n",
      "topic_7: самолёт военный корабль турист район операция армия технология фонд помощь обнаружить система данные область экипаж\n",
      "topic_8: учёный проект исследование земля первый новый день спутник станция программа москва место участник центр русский\n",
      "topic_9: всё очень гражданин большой университет жизнь первый животное хороший новый миссия друг женщина пенсия россия\n"
     ]
    }
   ],
   "source": [
    "# просмотр списка тем из полученной модели\n",
    "\n",
    "x=lda.show_topics(num_topics=N_topic, num_words=15,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    " \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.771234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.638109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.670467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113932</td>\n",
       "      <td>0.128576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id   topic_0   topic_1   topic_2  topic_3   topic_4  topic_5   topic_6  \\\n",
       "0       6  0.771234  0.000000  0.000000      0.0  0.129235      0.0  0.092844   \n",
       "1    4896  0.638109  0.000000  0.000000      0.0  0.340806      0.0  0.000000   \n",
       "2    4897  0.000000  0.070815  0.000000      0.0  0.670467      0.0  0.000000   \n",
       "3    4898  0.000000  0.000000  0.000000      0.0  0.157088      0.0  0.038497   \n",
       "4    4899  0.000000  0.000000  0.264416      0.0  0.000000      0.0  0.000000   \n",
       "\n",
       "    topic_7   topic_8   topic_9  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.113932  0.128576  \n",
       "3  0.000000  0.000000  0.795411  \n",
       "4  0.307397  0.000000  0.407499  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создаём матрицу тем\n",
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(N_topic)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(N_topic)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь распределения текстов-тем\n",
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(N_topic)]].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEDIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_user_embedding(user_articles_list):\n",
    "    user_articles_list = ast.literal_eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector1 = np.median(user_vector, 0)\n",
    "    return user_vector1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "user_embeddings.columns = ['topic_{}'.format(i) for i in range(N_topic)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(N_topic)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.182512</td>\n",
       "      <td>0.044483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176407</td>\n",
       "      <td>0.117138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>0.179448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>0.083767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259687</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>0.053066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>0.218373</td>\n",
       "      <td>0.006106</td>\n",
       "      <td>0.080470</td>\n",
       "      <td>0.108476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236620</td>\n",
       "      <td>0.115035</td>\n",
       "      <td>0.052760</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid   topic_0   topic_1   topic_2   topic_3  topic_4  topic_5  \\\n",
       "0  u105138  0.000000  0.015514  0.182512  0.044483      0.0      0.0   \n",
       "1  u108690  0.179448  0.000000  0.198227  0.083767      0.0      0.0   \n",
       "2  u108339  0.218373  0.006106  0.080470  0.108476      0.0      0.0   \n",
       "\n",
       "    topic_6   topic_7   topic_8   topic_9  churn  \n",
       "0  0.074867  0.000000  0.176407  0.117138      0  \n",
       "1  0.259687  0.021152  0.032242  0.053066      1  \n",
       "2  0.236620  0.115035  0.052760  0.020640      1  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.merge(user_embeddings, target, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(N_topic)]], \n",
    "                                                    X['churn'], random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучим модель\n",
    "logreg = LogisticRegression(C=1.0)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0393335 , 0.10496361, 0.92822041, 0.1481409 , 0.13181818,\n",
       "       0.18433829, 0.26201603, 0.10949956, 0.07534441, 0.02121908])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получаем прогноз для тестовой выборки\n",
    "preds = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.288260, F-Score=0.723, Precision=0.658, Recall=0.803\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test.values[:1000], preds[:1000])\n",
    "\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Type': ['median'], 'F-Score': [fscore[ix]], 'Precision': [precision[ix]], 'Recall': [recall[ix]], 'Roc_auc_score': [roc_auc]}\n",
    "median_embedding = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>median_embedding</td>\n",
       "      <td>0.723247</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.959923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type   F-Score  Precision    Recall  Roc_auc_score\n",
       "0  median_embedding  0.723247   0.657718  0.803279       0.959923"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.421572, F-Score=0.647, Precision=0.682, Recall=0.615\n"
     ]
    }
   ],
   "source": [
    "def get_user_embedding(user_articles_list):\n",
    "    user_articles_list = ast.literal_eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector1 = np.max(user_vector, 0)\n",
    "    return user_vector1\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "user_embeddings.columns = ['topic_{}'.format(i) for i in range(N_topic)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(N_topic)]]\n",
    "\n",
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "# разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(N_topic)]], \n",
    "                                                    X['churn'], random_state=0, test_size=0.25)\n",
    "\n",
    "# обучим модель\n",
    "logreg = LogisticRegression(C=1.0)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# получаем прогноз для тестовой выборки\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.values[:1000], preds[:1000])\n",
    "\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "data = {'Type': ['max'], 'F-Score': [fscore[ix]], 'Precision': [precision[ix]], 'Recall': [recall[ix]], 'Roc_auc_score': [roc_auc]}\n",
    "max_embedding = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.351581, F-Score=0.697, Precision=0.706, Recall=0.689\n"
     ]
    }
   ],
   "source": [
    "def get_user_embedding(user_articles_list):\n",
    "    user_articles_list = ast.literal_eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector1 = np.mean(user_vector, 0)\n",
    "    return user_vector1\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "user_embeddings.columns = ['topic_{}'.format(i) for i in range(N_topic)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(N_topic)]]\n",
    "\n",
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "# разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(N_topic)]], \n",
    "                                                    X['churn'], random_state=0, test_size=0.25)\n",
    "\n",
    "# обучим модель\n",
    "logreg = LogisticRegression(C=1.0)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# получаем прогноз для тестовой выборки\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.values[:1000], preds[:1000])\n",
    "\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "data = {'Type': ['mean'], 'F-Score': [fscore[ix]], 'Precision': [precision[ix]], 'Recall': [recall[ix]], 'Roc_auc_score': [roc_auc]}\n",
    "mean_embedding = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравниваем результаты\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>median</td>\n",
       "      <td>0.723247</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.959923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.697095</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.958244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max</td>\n",
       "      <td>0.646552</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.614754</td>\n",
       "      <td>0.935817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type   F-Score  Precision    Recall  Roc_auc_score\n",
       "0  median  0.723247   0.657718  0.803279       0.959923\n",
       "0    mean  0.697095   0.705882  0.688525       0.958244\n",
       "0     max  0.646552   0.681818  0.614754       0.935817"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([median_embedding, mean_embedding, max_embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
